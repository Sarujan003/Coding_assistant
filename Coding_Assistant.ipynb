{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# --- Constants ---\n",
        "OLLAMA_URL = \"https://evolving-sailfish-closing.ngrok-free.app\"\n",
        "MODEL_NAME = \"qwen2.5-coder\"\n",
        "\n",
        "SYSTEM_PROMPT = {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": (\n",
        "        \"You are a professional AI coding assistant. \"\n",
        "        \"Be concise, technically accurate, and helpful. \"\n",
        "        \"Use clean code, best practices, and include examples where appropriate.\"\n",
        "    )\n",
        "}\n",
        "\n",
        "# --- Core Chat Logic ---\n",
        "\n",
        "def create_new_chat():\n",
        "    \"\"\"Creates a new chat history with the system prompt.\"\"\"\n",
        "    return [SYSTEM_PROMPT.copy()]\n",
        "\n",
        "def chat_with_ollama(user_message, history):\n",
        "    \"\"\"Streams a response from the Ollama API and updates the history.\"\"\"\n",
        "    history.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            f\"{OLLAMA_URL}/api/chat\",\n",
        "            json={\"model\": MODEL_NAME, \"messages\": history, \"stream\": True},\n",
        "            stream=True,\n",
        "            timeout=120,\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "\n",
        "        assistant_reply = \"\"\n",
        "        for line in response.iter_lines():\n",
        "            if line:\n",
        "                try:\n",
        "                    data = json.loads(line.decode(\"utf-8\"))\n",
        "                    if \"message\" in data and \"content\" in data[\"message\"]:\n",
        "                        token = data[\"message\"][\"content\"]\n",
        "                        assistant_reply += token\n",
        "                        yield history + [{\"role\": \"assistant\", \"content\": assistant_reply}]\n",
        "                    if data.get(\"done\"):\n",
        "                        break\n",
        "                except json.JSONDecodeError:\n",
        "                    print(f\"Warning: Could not decode JSON line: {line}\")\n",
        "                    continue\n",
        "\n",
        "        history.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        error_message = f\"API Error: Could not connect to Ollama. Details: {e}\"\n",
        "        history.append({\"role\": \"assistant\", \"content\": error_message})\n",
        "        yield history\n",
        "\n",
        "# --- Helper Function for UI ---\n",
        "\n",
        "def format_chat_for_display(history):\n",
        "    \"\"\"Filters out the system prompt for display in the chatbot UI.\"\"\"\n",
        "    return [m for m in history if m.get(\"role\") != \"system\"]\n",
        "\n",
        "# --- Gradio UI and Event Handlers ---\n",
        "\n",
        "css = \"\"\"\n",
        "/* Force the radio buttons into a single vertical column */\n",
        ".chat-list-radio .gr-form {\n",
        "    display: flex;\n",
        "    flex-direction: column !important;\n",
        "    align-items: stretch !important;\n",
        "}\n",
        "\n",
        "/* Set a maximum width for the sidebar column */\n",
        "#left-sidebar {\n",
        "    max-width: 300px !important;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft(), css=css) as demo:\n",
        "    # State components to hold session data\n",
        "    chat_sessions = gr.State([create_new_chat()])\n",
        "    chat_names = gr.State([\"Chat 1\"])\n",
        "    selected_chat_idx = gr.State(0)\n",
        "\n",
        "    gr.Markdown(\"# Coding Assistant\")\n",
        "\n",
        "    with gr.Row(equal_height=False):\n",
        "        # --- Left Sidebar ---\n",
        "        with gr.Column(scale=0.5, min_width=200, elem_id=\"left-sidebar\"):\n",
        "            # Move the button to the top of the sidebar for better UI\n",
        "            new_chat_btn = gr.Button(\"âž• New Chat\", variant=\"secondary\")\n",
        "\n",
        "            gr.Markdown(\"### Chats\")\n",
        "            chat_list = gr.Radio(\n",
        "                choices=[\"Chat 1\"],\n",
        "                value=\"Chat 1\",\n",
        "                label=\"Chat Sessions\",\n",
        "                show_label=False,\n",
        "                interactive=True,\n",
        "                elem_classes=[\"chat-list-radio\"]\n",
        "            )\n",
        "\n",
        "        # --- Main Chat Area ---\n",
        "        with gr.Column(scale=3.5):\n",
        "            chatbot = gr.Chatbot(\n",
        "                type=\"messages\",\n",
        "                label=f\"Model: {MODEL_NAME}\",\n",
        "                bubble_full_width=False,\n",
        "                height=500,\n",
        "                render_markdown=True,\n",
        "                latex_delimiters=[{\"left\": \"$$\", \"right\": \"$$\", \"display\": True}, {\"left\": \"$\", \"right\": \"$\", \"display\": False}],\n",
        "            )\n",
        "            with gr.Row():\n",
        "                msg_textbox = gr.Textbox(\n",
        "                    show_label=False,\n",
        "                    placeholder=\"Ask a coding question...\",\n",
        "                    scale=7,\n",
        "                    autofocus=True,\n",
        "                )\n",
        "                send_btn = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
        "\n",
        "    # --- Event Handler Functions ---\n",
        "\n",
        "    def add_new_chat(sessions, names):\n",
        "        sessions.insert(0, create_new_chat())\n",
        "        new_name = f\"Chat {len(sessions)}\"\n",
        "        names.insert(0, new_name)\n",
        "        new_idx = 0\n",
        "        return (\n",
        "            sessions, names, new_idx, [], gr.update(choices=names, value=names[0])\n",
        "        )\n",
        "\n",
        "    def switch_chat(choice, sessions, names):\n",
        "        if not choice or choice not in names:\n",
        "            idx = 0\n",
        "        else:\n",
        "            idx = names.index(choice)\n",
        "        history = sessions[idx]\n",
        "        return idx, format_chat_for_display(history)\n",
        "\n",
        "    def handle_user_message(user_message, sessions, current_idx):\n",
        "        if not user_message.strip():\n",
        "            yield \"\", format_chat_for_display(sessions[current_idx]), sessions\n",
        "            return\n",
        "\n",
        "        history = sessions[current_idx]\n",
        "        yield \"\", format_chat_for_display(history) + [{\"role\": \"user\", \"content\": user_message}], sessions\n",
        "\n",
        "        stream = chat_with_ollama(user_message, history)\n",
        "        for partial_history in stream:\n",
        "            sessions[current_idx] = partial_history\n",
        "            yield \"\", format_chat_for_display(partial_history), sessions\n",
        "\n",
        "    # --- Connecting Handlers to Components ---\n",
        "\n",
        "    new_chat_btn.click(\n",
        "        fn=add_new_chat,\n",
        "        inputs=[chat_sessions, chat_names],\n",
        "        outputs=[chat_sessions, chat_names, selected_chat_idx, chatbot, chat_list]\n",
        "    )\n",
        "\n",
        "    chat_list.input(\n",
        "        fn=switch_chat,\n",
        "        inputs=[chat_list, chat_sessions, chat_names],\n",
        "        outputs=[selected_chat_idx, chatbot]\n",
        "    )\n",
        "\n",
        "    submit_event = [send_btn.click, msg_textbox.submit]\n",
        "    for event in submit_event:\n",
        "        event(\n",
        "            fn=handle_user_message,\n",
        "            inputs=[msg_textbox, chat_sessions, selected_chat_idx],\n",
        "            outputs=[msg_textbox, chatbot, chat_sessions],\n",
        "            show_progress=\"hidden\"\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        },
        "collapsed": true,
        "id": "KgCxzRSnynG4",
        "outputId": "cea7a899-5f30-43a2-d769-1efc862545c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/layouts/column.py:59: UserWarning: 'scale' value should be an integer. Using 0.5 will cause issues.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/layouts/column.py:59: UserWarning: 'scale' value should be an integer. Using 3.5 will cause issues.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-2-912187176.py:107: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://6ce64ba5f69f275841.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6ce64ba5f69f275841.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://6ce64ba5f69f275841.gradio.live\n"
          ]
        }
      ]
    }
  ]
}