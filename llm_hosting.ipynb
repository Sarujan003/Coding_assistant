{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYcCrBW99eUU",
        "outputId": "2e54f413-3d0f-4c97-ef13-de592d35267a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "--2025-06-28 08:11:00--  https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 75.2.60.68, 35.71.179.82, 99.83.220.108, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|75.2.60.68|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9254557 (8.8M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-v3-stable-linux-amd64.tgz’\n",
            "\n",
            "ngrok-v3-stable-lin 100%[===================>]   8.83M  3.93MB/s    in 2.2s    \n",
            "\n",
            "2025-06-28 08:11:04 (3.93 MB/s) - ‘ngrok-v3-stable-linux-amd64.tgz’ saved [9254557/9254557]\n",
            "\n",
            "ngrok\n"
          ]
        }
      ],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n",
        "!tar xvzf ngrok-v3-stable-linux-amd64.tgz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"AUTHTOKEN\"] = userdata.get('AUTHTOKEN')"
      ],
      "metadata": {
        "id": "44g5c7Vvl_n1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ex33IgOD9hfs",
        "outputId": "95e8a851-c503-4e79-bc94-bbccb86ebea5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "\u001b[32mINFO\u001b[0m[06-28|10:17:52] no configuration paths supplied \n",
            "\u001b[32mINFO\u001b[0m[06-28|10:17:52] using configuration at default config path \u001b[32mpath\u001b[0m=/root/.config/ngrok/ngrok.yml\n",
            "\u001b[32mINFO\u001b[0m[06-28|10:17:52] open config file                         \u001b[32mpath\u001b[0m=/root/.config/ngrok/ngrok.yml \u001b[32merr\u001b[0m=nil\n",
            "time=2025-06-28T10:17:52.295Z level=INFO source=routes.go:1235 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
            "time=2025-06-28T10:17:52.296Z level=INFO source=images.go:476 msg=\"total blobs: 10\"\n",
            "time=2025-06-28T10:17:52.296Z level=INFO source=images.go:483 msg=\"total unused blobs removed: 0\"\n",
            "time=2025-06-28T10:17:52.296Z level=INFO source=routes.go:1288 msg=\"Listening on 127.0.0.1:11434 (version 0.9.3)\"\n",
            "time=2025-06-28T10:17:52.297Z level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\n",
            "t=2025-06-28T10:17:52+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n",
            "t=2025-06-28T10:17:52+0000 lvl=info msg=\"client session established\" obj=tunnels.session\n",
            "t=2025-06-28T10:17:52+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "t=2025-06-28T10:17:52+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=command_line addr=http://localhost:11434 url=https://evolving-sailfish-closing.ngrok-free.app\n",
            "time=2025-06-28T10:17:52.476Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-0b9ae838-54e3-e23a-60d3-503e4e5883fd library=cuda variant=v12 compute=7.5 driver=12.4 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"14.6 GiB\"\n",
            "[GIN] 2025/06/28 - 10:17:57 |\u001b[97;42m 200 \u001b[0m|      70.594µs |       127.0.0.1 |\u001b[97;45m HEAD    \u001b[0m \"/\"\n",
            "[GIN] 2025/06/28 - 10:17:57 |\u001b[97;42m 200 \u001b[0m|   69.757454ms |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/show\"\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026ltime=2025-06-28T10:17:57.569Z level=INFO source=sched.go:788 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 gpu=GPU-0b9ae838-54e3-e23a-60d3-503e4e5883fd parallel=2 available=15720382464 required=\"5.6 GiB\"\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026ltime=2025-06-28T10:17:57.658Z level=INFO source=server.go:135 msg=\"system memory\" total=\"12.7 GiB\" free=\"11.5 GiB\" free_swap=\"0 B\"\n",
            "time=2025-06-28T10:17:57.658Z level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=29 layers.split=\"\" memory.available=\"[14.6 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"5.6 GiB\" memory.required.partial=\"5.6 GiB\" memory.required.kv=\"448.0 MiB\" memory.required.allocations=\"[5.6 GiB]\" memory.weights.total=\"4.1 GiB\" memory.weights.repeating=\"3.7 GiB\" memory.weights.nonrepeating=\"426.4 MiB\" memory.graph.full=\"478.0 MiB\" memory.graph.partial=\"730.4 MiB\"\n",
            "llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from /root/.ollama/models/blobs/sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 7B\n",
            "llama_model_loader: - kv   6:                            general.license str              = apache-2.0\n",
            "llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...\n",
            "llama_model_loader: - kv   8:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B\n",
            "llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen\n",
            "llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...\n",
            "llama_model_loader: - kv  12:                               general.tags arr[str,6]       = [\"code\", \"codeqwen\", \"chat\", \"qwen\", ...\n",
            "llama_model_loader: - kv  13:                          general.languages arr[str,1]       = [\"en\"]\n",
            "llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28\n",
            "llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768\n",
            "llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584\n",
            "llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944\n",
            "llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28\n",
            "llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4\n",
            "llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  22:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026lllama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645\n",
            "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643\n",
            "llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
            "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  141 tensors\n",
            "llama_model_loader: - type q4_K:  169 tensors\n",
            "llama_model_loader: - type q6_K:   29 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 4.36 GiB (4.91 BPW) \n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026lload: special tokens cache size = 22\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026lload: token to piece cache size = 0.9310 MB\n",
            "print_info: arch             = qwen2\n",
            "print_info: vocab_only       = 1\n",
            "print_info: model type       = ?B\n",
            "print_info: model params     = 7.62 B\n",
            "print_info: general.name     = Qwen2.5 Coder 7B Instruct\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 152064\n",
            "print_info: n_merges         = 151387\n",
            "print_info: BOS token        = 151643 '<|endoftext|>'\n",
            "print_info: EOS token        = 151645 '<|im_end|>'\n",
            "print_info: EOT token        = 151645 '<|im_end|>'\n",
            "print_info: PAD token        = 151643 '<|endoftext|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
            "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
            "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
            "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
            "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
            "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
            "print_info: EOG token        = 151643 '<|endoftext|>'\n",
            "print_info: EOG token        = 151645 '<|im_end|>'\n",
            "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
            "print_info: EOG token        = 151663 '<|repo_name|>'\n",
            "print_info: EOG token        = 151664 '<|file_sep|>'\n",
            "print_info: max token length = 256\n",
            "llama_model_load: vocab only - skipping tensors\n",
            "time=2025-06-28T10:17:57.937Z level=INFO source=server.go:438 msg=\"starting llama server\" cmd=\"/usr/local/bin/ollama runner --model /root/.ollama/models/blobs/sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --threads 1 --parallel 2 --port 41087\"\n",
            "time=2025-06-28T10:17:57.938Z level=INFO source=sched.go:483 msg=\"loaded runners\" count=1\n",
            "time=2025-06-28T10:17:57.938Z level=INFO source=server.go:598 msg=\"waiting for llama runner to start responding\"\n",
            "time=2025-06-28T10:17:57.938Z level=INFO source=server.go:632 msg=\"waiting for server to become available\" status=\"llm server not responding\"\n",
            "time=2025-06-28T10:17:57.953Z level=INFO source=runner.go:815 msg=\"starting go runner\"\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "load_backend: loaded CUDA backend from /usr/local/lib/ollama/libggml-cuda.so\n",
            "load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-skylakex.so\n",
            "time=2025-06-28T10:17:58.026Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\n",
            "time=2025-06-28T10:17:58.029Z level=INFO source=runner.go:874 msg=\"Server listening on 127.0.0.1:41087\"\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026lllama_model_load_from_file_impl: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026lllama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from /root/.ollama/models/blobs/sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 7B\n",
            "llama_model_loader: - kv   6:                            general.license str              = apache-2.0\n",
            "llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...\n",
            "llama_model_loader: - kv   8:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B\n",
            "llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen\n",
            "llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...\n",
            "llama_model_loader: - kv  12:                               general.tags arr[str,6]       = [\"code\", \"codeqwen\", \"chat\", \"qwen\", ...\n",
            "llama_model_loader: - kv  13:                          general.languages arr[str,1]       = [\"en\"]\n",
            "llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28\n",
            "llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768\n",
            "llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584\n",
            "llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944\n",
            "llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28\n",
            "llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4\n",
            "llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  22:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645\n",
            "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643\n",
            "llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
            "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  141 tensors\n",
            "llama_model_loader: - type q4_K:  169 tensors\n",
            "llama_model_loader: - type q6_K:   29 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 4.36 GiB (4.91 BPW) \n",
            "time=2025-06-28T10:17:58.189Z level=INFO source=server.go:632 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026lload: special tokens cache size = 22\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026lload: token to piece cache size = 0.9310 MB\n",
            "print_info: arch             = qwen2\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 3584\n",
            "print_info: n_layer          = 28\n",
            "print_info: n_head           = 28\n",
            "print_info: n_head_kv        = 4\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_swa_pattern    = 1\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 7\n",
            "print_info: n_embd_k_gqa     = 512\n",
            "print_info: n_embd_v_gqa     = 512\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 18944\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = -1\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 7B\n",
            "print_info: model params     = 7.62 B\n",
            "print_info: general.name     = Qwen2.5 Coder 7B Instruct\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 152064\n",
            "print_info: n_merges         = 151387\n",
            "print_info: BOS token        = 151643 '<|endoftext|>'\n",
            "print_info: EOS token        = 151645 '<|im_end|>'\n",
            "print_info: EOT token        = 151645 '<|im_end|>'\n",
            "print_info: PAD token        = 151643 '<|endoftext|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
            "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
            "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
            "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
            "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
            "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
            "print_info: EOG token        = 151643 '<|endoftext|>'\n",
            "print_info: EOG token        = 151645 '<|im_end|>'\n",
            "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
            "print_info: EOG token        = 151663 '<|repo_name|>'\n",
            "print_info: EOG token        = 151664 '<|file_sep|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026lload_tensors: offloading 28 repeating layers to GPU\n",
            "load_tensors: offloading output layer to GPU\n",
            "load_tensors: offloaded 29/29 layers to GPU\n",
            "load_tensors:        CUDA0 model buffer size =  4168.09 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =   292.36 MiB\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026lllama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 2\n",
            "llama_context: n_ctx         = 8192\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 1024\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: freq_base     = 1000000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "llama_context:  CUDA_Host  output buffer size =     1.19 MiB\n",
            "llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026lllama_kv_cache_unified:      CUDA0 KV buffer size =   448.00 MiB\n",
            "llama_kv_cache_unified: KV self size  =  448.00 MiB, K (f16):  224.00 MiB, V (f16):  224.00 MiB\n",
            "llama_context:      CUDA0 compute buffer size =   492.00 MiB\n",
            "llama_context:  CUDA_Host compute buffer size =    23.01 MiB\n",
            "llama_context: graph nodes  = 1042\n",
            "llama_context: graph splits = 2\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026ltime=2025-06-28T10:17:59.946Z level=INFO source=server.go:637 msg=\"llama runner started in 2.01 seconds\"\n",
            "[GIN] 2025/06/28 - 10:17:59 |\u001b[97;42m 200 \u001b[0m|  2.616419159s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
            "\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25h\u001b[?2004h>>> \u001b[38;5;245mSend a message (/? for help)\u001b[28D\u001b[0mt=2025-06-28T10:19:54+0000 lvl=info msg=\"join connections\" obj=join id=75b16b2c2e3a l=127.0.0.1:11434 r=34.169.235.153:54384\n",
            "[GIN] 2025/06/28 - 10:19:56 |\u001b[97;42m 200 \u001b[0m|  1.766630647s |  34.169.235.153 |\u001b[97;46m POST    \u001b[0m \"/api/chat\"\n",
            "t=2025-06-28T10:20:05+0000 lvl=info msg=\"join connections\" obj=join id=a4c2e9e16dfe l=127.0.0.1:11434 r=34.169.235.153:35258\n",
            "[GIN] 2025/06/28 - 10:20:20 |\u001b[97;42m 200 \u001b[0m| 15.020942527s |  34.169.235.153 |\u001b[97;46m POST    \u001b[0m \"/api/chat\"\n",
            "t=2025-06-28T10:20:36+0000 lvl=info msg=\"join connections\" obj=join id=e5db63b84167 l=127.0.0.1:11434 r=34.169.235.153:34218\n",
            "[GIN] 2025/06/28 - 10:20:50 |\u001b[97;42m 200 \u001b[0m|  13.76763198s |  34.169.235.153 |\u001b[97;46m POST    \u001b[0m \"/api/chat\"\n",
            "t=2025-06-28T10:21:20+0000 lvl=info msg=\"received stop request\" obj=app stopReq=\"{err:<nil> restart:false}\"\n",
            "t=2025-06-28T10:21:20+0000 lvl=info msg=\"session closing\" obj=tunnels.session err=nil\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!./ngrok config add-authtoken {os.environ[\"AUTHTOKEN\"]}\n",
        "!ollama serve & ./ngrok http 11434 --host-header=\"localhost:11434\" --domain=\"evolving-sailfish-closing.ngrok-free.app\" --log stdout & sleep 5s && ollama run qwen2.5-coder"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}